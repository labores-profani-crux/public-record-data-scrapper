# Phase 3: Backend Infrastructure - Detailed Task Breakdown

**Duration**: 4 weeks (Weeks 9-12)
**Goal**: Production-ready backend with database
**Priority**: HIGH
**Dependencies**: Phases 1-2 complete

---

## Week 9-10: Database & API Server

### Task 3.1: PostgreSQL Database Setup
**Assignee**: TBD
**Effort**: 3 days
**Priority**: CRITICAL

#### Subtask 3.1.1: Cloud Database Provisioning
**Time**: 1 day

**Option A: AWS RDS PostgreSQL**
```bash
# Using AWS CLI
aws rds create-db-instance \
  --db-instance-identifier ucc-intelligence-prod \
  --db-instance-class db.m5.large \
  --engine postgres \
  --engine-version 15.4 \
  --master-username admin \
  --master-user-password [SECURE_PASSWORD] \
  --allocated-storage 100 \
  --storage-type gp3 \
  --storage-encrypted \
  --backup-retention-period 7 \
  --preferred-backup-window "03:00-04:00" \
  --multi-az \
  --publicly-accessible false \
  --vpc-security-group-ids sg-xxxxx \
  --db-subnet-group-name ucc-db-subnet-group
```

**Option B: Google Cloud SQL**
```bash
# Using gcloud CLI
gcloud sql instances create ucc-intelligence-prod \
  --database-version=POSTGRES_15 \
  --tier=db-custom-4-16384 \
  --region=us-central1 \
  --network=default \
  --backup \
  --backup-start-time=03:00 \
  --database-flags=max_connections=200 \
  --availability-type=REGIONAL \
  --storage-size=100GB \
  --storage-type=SSD \
  --storage-auto-increase
```

**Configuration Parameters:**
```sql
-- PostgreSQL Configuration (postgresql.conf)
max_connections = 200
shared_buffers = 4GB
effective_cache_size = 12GB
maintenance_work_mem = 1GB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
random_page_cost = 1.1
effective_io_concurrency = 200
work_mem = 10485kB
min_wal_size = 2GB
max_wal_size = 8GB
max_worker_processes = 4
max_parallel_workers_per_gather = 2
max_parallel_workers = 4
```

**Acceptance Criteria:**
- [ ] Database instance provisioned
- [ ] Multi-AZ/Regional setup for high availability
- [ ] Encryption at rest enabled
- [ ] Automated backups configured (daily, 7-day retention)
- [ ] Connection limits set (200 max)
- [ ] Performance parameters tuned

---

#### Subtask 3.1.2: PgBouncer Connection Pooling
**Time**: 0.5 days

**Install PgBouncer:**
```bash
# On application server
sudo apt-get install pgbouncer

# Configure /etc/pgbouncer/pgbouncer.ini
[databases]
ucc_intelligence = host=ucc-db.xxxxx.rds.amazonaws.com port=5432 dbname=ucc_intelligence

[pgbouncer]
listen_port = 6432
listen_addr = 127.0.0.1
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 25
min_pool_size = 10
reserve_pool_size = 5
reserve_pool_timeout = 3
max_db_connections = 100
max_user_connections = 100
server_idle_timeout = 600
server_lifetime = 3600
server_connect_timeout = 15
query_timeout = 0
client_idle_timeout = 0
```

**Acceptance Criteria:**
- [ ] PgBouncer installed and configured
- [ ] Connection pooling working (25 pool size)
- [ ] Transaction pooling mode
- [ ] Connection limits enforced

---

#### Subtask 3.1.3: Database Schema Migration
**File**: `database/migrations/001_initial_schema.sql`
**Time**: 1 day

```sql
-- Run existing schema from database/schema.sql
\i database/schema.sql

-- Verify tables created
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'public';

-- Expected output: 11 tables
-- ucc_filings, prospects, prospect_ucc_filings, growth_signals,
-- health_scores, competitors, portfolio_companies, users,
-- usage_tracking, data_sources, audit_logs

-- Verify indexes
SELECT indexname, tablename
FROM pg_indexes
WHERE schemaname = 'public'
ORDER BY tablename, indexname;

-- Expected: 35+ indexes

-- Test constraints
INSERT INTO prospects (company_name, state, status)
VALUES ('Test Company', 'NY', 'active');
-- Should succeed

INSERT INTO prospects (company_name, state, status)
VALUES ('Test Company', 'INVALID', 'active');
-- Should fail (check constraint on state)

-- Verify triggers
SELECT trigger_name, event_object_table, action_statement
FROM information_schema.triggers
WHERE trigger_schema = 'public';
```

**Create Migration Tool:**
**File**: `database/migrate.ts`

```typescript
import { Client } from 'pg'
import fs from 'fs'
import path from 'path'

interface Migration {
  version: number
  name: string
  sql: string
}

class MigrationRunner {
  private client: Client

  constructor(connectionString: string) {
    this.client = new Client({ connectionString })
  }

  async connect(): Promise<void> {
    await this.client.connect()
    await this.createMigrationsTable()
  }

  async disconnect(): Promise<void> {
    await this.client.end()
  }

  private async createMigrationsTable(): Promise<void> {
    await this.client.query(`
      CREATE TABLE IF NOT EXISTS schema_migrations (
        version INTEGER PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `)
  }

  async getAppliedMigrations(): Promise<number[]> {
    const result = await this.client.query(
      'SELECT version FROM schema_migrations ORDER BY version'
    )
    return result.rows.map(row => row.version)
  }

  async getPendingMigrations(): Promise<Migration[]> {
    const appliedVersions = await this.getAppliedMigrations()
    const allMigrations = this.loadMigrations()

    return allMigrations.filter(m => !appliedVersions.includes(m.version))
  }

  private loadMigrations(): Migration[] {
    const migrationsDir = path.join(__dirname, 'migrations')
    const files = fs.readdirSync(migrationsDir)
      .filter(f => f.endsWith('.sql'))
      .sort()

    return files.map(file => {
      const match = file.match(/^(\d+)_(.+)\.sql$/)
      if (!match) throw new Error(`Invalid migration filename: ${file}`)

      const version = parseInt(match[1])
      const name = match[2]
      const sql = fs.readFileSync(path.join(migrationsDir, file), 'utf-8')

      return { version, name, sql }
    })
  }

  async runMigrations(): Promise<void> {
    const pending = await this.getPendingMigrations()

    if (pending.length === 0) {
      console.log('No pending migrations')
      return
    }

    for (const migration of pending) {
      console.log(`Applying migration ${migration.version}: ${migration.name}`)

      try {
        await this.client.query('BEGIN')
        await this.client.query(migration.sql)
        await this.client.query(
          'INSERT INTO schema_migrations (version, name) VALUES ($1, $2)',
          [migration.version, migration.name]
        )
        await this.client.query('COMMIT')

        console.log(`✓ Migration ${migration.version} applied successfully`)

      } catch (error) {
        await this.client.query('ROLLBACK')
        console.error(`✗ Migration ${migration.version} failed:`, error)
        throw error
      }
    }
  }

  async rollback(steps = 1): Promise<void> {
    const applied = await this.getAppliedMigrations()
    const toRollback = applied.slice(-steps)

    for (const version of toRollback.reverse()) {
      console.log(`Rolling back migration ${version}`)

      const downFile = path.join(__dirname, 'migrations', `${version}_down.sql`)
      if (!fs.existsSync(downFile)) {
        throw new Error(`No rollback script found for migration ${version}`)
      }

      const sql = fs.readFileSync(downFile, 'utf-8')

      try {
        await this.client.query('BEGIN')
        await this.client.query(sql)
        await this.client.query(
          'DELETE FROM schema_migrations WHERE version = $1',
          [version]
        )
        await this.client.query('COMMIT')

        console.log(`✓ Migration ${version} rolled back successfully`)

      } catch (error) {
        await this.client.query('ROLLBACK')
        console.error(`✗ Rollback of migration ${version} failed:`, error)
        throw error
      }
    }
  }
}

// CLI
if (require.main === module) {
  const command = process.argv[2]
  const connectionString = process.env.DATABASE_URL

  if (!connectionString) {
    console.error('DATABASE_URL environment variable not set')
    process.exit(1)
  }

  const runner = new MigrationRunner(connectionString)

  ;(async () => {
    await runner.connect()

    try {
      switch (command) {
        case 'up':
          await runner.runMigrations()
          break
        case 'down':
          const steps = parseInt(process.argv[3] || '1')
          await runner.rollback(steps)
          break
        case 'status':
          const pending = await runner.getPendingMigrations()
          console.log(`Pending migrations: ${pending.length}`)
          pending.forEach(m => console.log(`  ${m.version}: ${m.name}`))
          break
        default:
          console.log('Usage: npm run migrate [up|down|status]')
      }
    } finally {
      await runner.disconnect()
    }
  })()
}
```

**Update package.json:**
```json
{
  "scripts": {
    "migrate": "tsx database/migrate.ts up",
    "migrate:down": "tsx database/migrate.ts down",
    "migrate:status": "tsx database/migrate.ts status"
  }
}
```

**Acceptance Criteria:**
- [ ] Schema migration run successfully
- [ ] All 11 tables created
- [ ] All 35+ indexes created
- [ ] Constraints working
- [ ] Triggers functional
- [ ] Migration tool tested
- [ ] Rollback capability verified

---

### Task 3.2: Express API Server
**Assignee**: TBD
**Effort**: 5 days
**Priority**: CRITICAL

#### Subtask 3.2.1: Express Server Setup
**File**: `server/index.ts`
**Time**: 1 day

```typescript
import express, { Express, Request, Response, NextFunction } from 'express'
import cors from 'cors'
import helmet from 'helmet'
import compression from 'compression'
import morgan from 'morgan'
import { Pool } from 'pg'
import { config } from './config'
import { errorHandler } from './middleware/errorHandler'
import { requestLogger } from './middleware/requestLogger'
import { rateLimiter } from './middleware/rateLimiter'

// Import routes
import prospectsRouter from './routes/prospects'
import competitorsRouter from './routes/competitors'
import portfolioRouter from './routes/portfolio'
import enrichmentRouter from './routes/enrichment'
import healthRouter from './routes/health'

export class Server {
  private app: Express
  private pool: Pool

  constructor() {
    this.app = express()
    this.pool = new Pool({
      connectionString: config.database.url,
      max: 20,
      idleTimeoutMillis: 30000,
      connectionTimeoutMillis: 2000,
    })

    this.setupMiddleware()
    this.setupRoutes()
    this.setupErrorHandling()
  }

  private setupMiddleware(): void {
    // Security
    this.app.use(helmet())
    this.app.use(cors({
      origin: config.cors.origin,
      credentials: true
    }))

    // Parsing
    this.app.use(express.json({ limit: '10mb' }))
    this.app.use(express.urlencoded({ extended: true, limit: '10mb' }))

    // Compression
    this.app.use(compression())

    // Logging
    this.app.use(morgan('combined'))
    this.app.use(requestLogger)

    // Rate limiting
    this.app.use(rateLimiter)

    // Database pool
    this.app.use((req: Request, res: Response, next: NextFunction) => {
      req.db = this.pool
      next()
    })
  }

  private setupRoutes(): void {
    // API routes
    this.app.use('/api/prospects', prospectsRouter)
    this.app.use('/api/competitors', competitorsRouter)
    this.app.use('/api/portfolio', portfolioRouter)
    this.app.use('/api/enrichment', enrichmentRouter)
    this.app.use('/api/health', healthRouter)

    // 404 handler
    this.app.use('*', (req, res) => {
      res.status(404).json({
        error: 'Not Found',
        message: `Route ${req.originalUrl} not found`
      })
    })
  }

  private setupErrorHandling(): void {
    this.app.use(errorHandler)
  }

  async start(): Promise<void> {
    const port = config.server.port

    // Test database connection
    try {
      await this.pool.query('SELECT NOW()')
      console.log('✓ Database connection established')
    } catch (error) {
      console.error('✗ Database connection failed:', error)
      process.exit(1)
    }

    this.app.listen(port, () => {
      console.log(`✓ Server running on port ${port}`)
      console.log(`  Environment: ${config.server.env}`)
      console.log(`  Database: ${config.database.url.split('@')[1]}`)
    })
  }

  async shutdown(): Promise<void> {
    console.log('Shutting down server...')
    await this.pool.end()
    console.log('✓ Database pool closed')
    process.exit(0)
  }
}

// Start server
const server = new Server()
server.start()

// Graceful shutdown
process.on('SIGTERM', () => server.shutdown())
process.on('SIGINT', () => server.shutdown())
```

**Config File**: `server/config.ts`

```typescript
export const config = {
  server: {
    port: parseInt(process.env.PORT || '3000'),
    env: process.env.NODE_ENV || 'development'
  },
  database: {
    url: process.env.DATABASE_URL || 'postgresql://localhost:5432/ucc_intelligence'
  },
  cors: {
    origin: process.env.CORS_ORIGIN?.split(',') || ['http://localhost:5173']
  },
  rateLimit: {
    windowMs: 15 * 60 * 1000, // 15 minutes
    max: 100 // requests per window
  },
  jwt: {
    secret: process.env.JWT_SECRET || 'development-secret',
    expiresIn: '7d'
  }
}
```

**Acceptance Criteria:**
- [ ] Express server runs
- [ ] Database connection verified
- [ ] Middleware configured
- [ ] CORS enabled
- [ ] Security headers (Helmet)
- [ ] Request logging
- [ ] Graceful shutdown

---

#### Subtask 3.2.2: Prospects API Routes
**File**: `server/routes/prospects.ts`
**Time**: 2 days

```typescript
import { Router, Request, Response } from 'express'
import { z } from 'zod'
import { validateRequest } from '../middleware/validateRequest'
import { ProspectsService } from '../services/ProspectsService'

const router = Router()
const prospectsService = new ProspectsService()

// Validation schemas
const createProspectSchema = z.object({
  company_name: z.string().min(1),
  state: z.string().length(2),
  industry: z.enum(['restaurant', 'retail', 'construction', 'healthcare', 'manufacturing', 'services', 'technology']),
  lien_amount: z.number().positive().optional(),
  filing_date: z.string().datetime().optional()
})

const updateProspectSchema = createProspectSchema.partial()

const querySchema = z.object({
  page: z.string().regex(/^\d+$/).transform(Number).optional(),
  limit: z.string().regex(/^\d+$/).transform(Number).optional(),
  state: z.string().length(2).optional(),
  industry: z.string().optional(),
  min_score: z.string().regex(/^\d+$/).transform(Number).optional(),
  max_score: z.string().regex(/^\d+$/).transform(Number).optional(),
  status: z.enum(['all', 'unclaimed', 'claimed', 'contacted']).optional(),
  sort_by: z.enum(['priority_score', 'created_at', 'company_name']).optional(),
  sort_order: z.enum(['asc', 'desc']).optional()
})

// GET /api/prospects - List prospects (paginated, filtered, sorted)
router.get('/', validateRequest({ query: querySchema }), async (req: Request, res: Response) => {
  try {
    const {
      page = 1,
      limit = 20,
      state,
      industry,
      min_score,
      max_score,
      status,
      sort_by = 'priority_score',
      sort_order = 'desc'
    } = req.query

    const result = await prospectsService.list({
      page: Number(page),
      limit: Number(limit),
      filters: {
        state,
        industry,
        minScore: min_score ? Number(min_score) : undefined,
        maxScore: max_score ? Number(max_score) : undefined,
        status
      },
      sort: {
        by: sort_by as string,
        order: sort_order as 'asc' | 'desc'
      }
    })

    res.json({
      prospects: result.prospects,
      pagination: {
        page: result.page,
        limit: result.limit,
        total: result.total,
        totalPages: Math.ceil(result.total / result.limit)
      }
    })
  } catch (error) {
    throw error
  }
})

// GET /api/prospects/:id - Get prospect details
router.get('/:id', async (req: Request, res: Response) => {
  try {
    const prospect = await prospectsService.getById(req.params.id)

    if (!prospect) {
      return res.status(404).json({
        error: 'Not Found',
        message: `Prospect ${req.params.id} not found`
      })
    }

    res.json(prospect)
  } catch (error) {
    throw error
  }
})

// POST /api/prospects - Create prospect
router.post('/', validateRequest({ body: createProspectSchema }), async (req: Request, res: Response) => {
  try {
    const prospect = await prospectsService.create(req.body)
    res.status(201).json(prospect)
  } catch (error) {
    throw error
  }
})

// PATCH /api/prospects/:id - Update prospect
router.patch('/:id', validateRequest({ body: updateProspectSchema }), async (req: Request, res: Response) => {
  try {
    const prospect = await prospectsService.update(req.params.id, req.body)

    if (!prospect) {
      return res.status(404).json({
        error: 'Not Found',
        message: `Prospect ${req.params.id} not found`
      })
    }

    res.json(prospect)
  } catch (error) {
    throw error
  }
})

// DELETE /api/prospects/:id - Delete prospect
router.delete('/:id', async (req: Request, res: Response) => {
  try {
    const deleted = await prospectsService.delete(req.params.id)

    if (!deleted) {
      return res.status(404).json({
        error: 'Not Found',
        message: `Prospect ${req.params.id} not found`
      })
    }

    res.status(204).send()
  } catch (error) {
    throw error
  }
})

// POST /api/prospects/:id/claim - Claim prospect
router.post('/:id/claim', async (req: Request, res: Response) => {
  try {
    const prospect = await prospectsService.claim(req.params.id, req.user.id)

    if (!prospect) {
      return res.status(404).json({
        error: 'Not Found',
        message: `Prospect ${req.params.id} not found`
      })
    }

    res.json(prospect)
  } catch (error) {
    throw error
  }
})

// POST /api/prospects/claim - Batch claim prospects
router.post('/claim', async (req: Request, res: Response) => {
  try {
    const { prospect_ids } = req.body

    if (!Array.isArray(prospect_ids) || prospect_ids.length === 0) {
      return res.status(400).json({
        error: 'Bad Request',
        message: 'prospect_ids must be a non-empty array'
      })
    }

    const results = await prospectsService.batchClaim(prospect_ids, req.user.id)

    res.json({
      claimed: results.filter(r => r.success).length,
      failed: results.filter(r => !r.success).length,
      results
    })
  } catch (error) {
    throw error
  }
})

// POST /api/prospects/export - Export prospects
router.post('/export', async (req: Request, res: Response) => {
  try {
    const { format, prospect_ids } = req.body

    if (!['csv', 'json', 'xlsx'].includes(format)) {
      return res.status(400).json({
        error: 'Bad Request',
        message: 'format must be csv, json, or xlsx'
      })
    }

    const data = await prospectsService.export(prospect_ids, format)

    res.setHeader('Content-Type', format === 'json' ? 'application/json' : 'text/csv')
    res.setHeader('Content-Disposition', `attachment; filename=prospects.${format}`)
    res.send(data)
  } catch (error) {
    throw error
  }
})

export default router
```

**Service Layer**: `server/services/ProspectsService.ts`

```typescript
import { Pool } from 'pg'
import type { Prospect, ProspectFilter } from '../../src/lib/types'

export class ProspectsService {
  constructor(private pool: Pool) {}

  async list(params: {
    page: number
    limit: number
    filters: ProspectFilter
    sort: { by: string; order: 'asc' | 'desc' }
  }) {
    const { page, limit, filters, sort } = params
    const offset = (page - 1) * limit

    // Build WHERE clause
    const conditions: string[] = []
    const values: any[] = []
    let paramCount = 1

    if (filters.state) {
      conditions.push(`state = $${paramCount++}`)
      values.push(filters.state)
    }

    if (filters.industry) {
      conditions.push(`industry = $${paramCount++}`)
      values.push(filters.industry)
    }

    if (filters.minScore !== undefined) {
      conditions.push(`priority_score >= $${paramCount++}`)
      values.push(filters.minScore)
    }

    if (filters.maxScore !== undefined) {
      conditions.push(`priority_score <= $${paramCount++}`)
      values.push(filters.maxScore)
    }

    if (filters.status && filters.status !== 'all') {
      conditions.push(`status = $${paramCount++}`)
      values.push(filters.status)
    }

    const whereClause = conditions.length > 0
      ? `WHERE ${conditions.join(' AND ')}`
      : ''

    // Query prospects
    const query = `
      SELECT * FROM prospects
      ${whereClause}
      ORDER BY ${sort.by} ${sort.order.toUpperCase()}
      LIMIT $${paramCount++} OFFSET $${paramCount++}
    `
    values.push(limit, offset)

    const result = await this.pool.query(query, values)

    // Get total count
    const countQuery = `SELECT COUNT(*) FROM prospects ${whereClause}`
    const countResult = await this.pool.query(countQuery, values.slice(0, -2))

    return {
      prospects: result.rows,
      page,
      limit,
      total: parseInt(countResult.rows[0].count)
    }
  }

  async getById(id: string): Promise<Prospect | null> {
    const result = await this.pool.query(
      'SELECT * FROM prospects WHERE id = $1',
      [id]
    )
    return result.rows[0] || null
  }

  async create(data: Partial<Prospect>): Promise<Prospect> {
    const result = await this.pool.query(
      `INSERT INTO prospects (company_name, state, industry, lien_amount, filing_date)
       VALUES ($1, $2, $3, $4, $5)
       RETURNING *`,
      [data.company_name, data.state, data.industry, data.lien_amount, data.filing_date]
    )
    return result.rows[0]
  }

  async update(id: string, data: Partial<Prospect>): Promise<Prospect | null> {
    // Build SET clause dynamically
    const fields = Object.keys(data)
    const setClause = fields.map((field, i) => `${field} = $${i + 2}`).join(', ')
    const values = [id, ...fields.map(f => data[f])]

    const result = await this.pool.query(
      `UPDATE prospects SET ${setClause} WHERE id = $1 RETURNING *`,
      values
    )
    return result.rows[0] || null
  }

  async delete(id: string): Promise<boolean> {
    const result = await this.pool.query(
      'DELETE FROM prospects WHERE id = $1',
      [id]
    )
    return result.rowCount! > 0
  }

  async claim(id: string, userId: string): Promise<Prospect | null> {
    const result = await this.pool.query(
      `UPDATE prospects
       SET status = 'claimed', claimed_by = $2, claimed_at = NOW()
       WHERE id = $1
       RETURNING *`,
      [id, userId]
    )
    return result.rows[0] || null
  }

  async batchClaim(ids: string[], userId: string): Promise<any[]> {
    // Implement batch claim logic
    return []
  }

  async export(ids: string[], format: string): Promise<string> {
    // Implement export logic
    return ''
  }
}
```

**Acceptance Criteria:**
- [ ] All CRUD operations working
- [ ] Pagination implemented
- [ ] Filtering by multiple criteria
- [ ] Sorting by multiple fields
- [ ] Validation with Zod schemas
- [ ] Batch operations
- [ ] Export functionality
- [ ] Error handling
- [ ] Tests written (80%+ coverage)

---

## Phase 3 continues with Week 11-12 tasks in similar detail...

---

## Phase 3 Completion Checklist

### Week 9-10: Database & API Server ✓
- [ ] PostgreSQL provisioned (RDS/Cloud SQL)
- [ ] PgBouncer connection pooling
- [ ] Schema migration successful
- [ ] Migration tool working
- [ ] Express server running
- [ ] Prospects API complete (8 endpoints)
- [ ] Competitors API complete
- [ ] Portfolio API complete
- [ ] Enrichment API complete
- [ ] Health check API
- [ ] Request validation (Zod)
- [ ] Error handling middleware
- [ ] API documentation (OpenAPI)

### Week 11-12: Job Queue & Scheduling ✓
- [ ] Redis cluster provisioned
- [ ] BullMQ job queue setup
- [ ] Worker processes running
- [ ] Ingestion jobs (daily)
- [ ] Enrichment jobs (6h)
- [ ] Health score updates (12h)
- [ ] Job monitoring dashboard
- [ ] Failed job retry logic
- [ ] Job metrics tracking

### Deliverables
- [ ] Production database operational
- [ ] REST API with 20+ endpoints
- [ ] Job queue processing tasks
- [ ] Automated scheduling
- [ ] API tests (80%+ coverage)
- [ ] OpenAPI documentation
- [ ] Deployment scripts

### Metrics
- **API Response Time**: Target <500ms P95
- **Database Queries**: Target <100ms P95
- **Job Processing**: 1000+ jobs/hour
- **Uptime**: Target 99.9%

---

**Total Effort**: 4 weeks
**Total Cost**: ~$24,000 (@ $150/hr)
**Next Phase**: Phase 4 - Security & Authentication
